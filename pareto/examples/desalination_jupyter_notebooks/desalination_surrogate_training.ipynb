{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Machine Learning Surrogate\n",
    "\n",
    "To train a machine learning surrogate, we need to import several essential libraries. We will use `numpy` and `pandas` for data manipulation, and `tensorflow` for constructing and training the neural network. Additionally, we will utilize the `KerasSurrogate` object from the `idaes.core.surrogate` module in IDAES for surrogate modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import IDAES libraries\n",
    "from idaes.core.surrogate.sampling.data_utils import split_training_validation\n",
    "from idaes.core.surrogate.sampling.scaling import OffsetScaler\n",
    "from idaes.core.surrogate.keras_surrogate import (\n",
    "    KerasSurrogate,\n",
    ")\n",
    "from idaes.core.surrogate.plotting.sm_plotter import (\n",
    "    surrogate_scatter2D,\n",
    "    surrogate_parity,\n",
    "    surrogate_residual,\n",
    ")\n",
    "\n",
    "# fix environment variables to ensure consist neural network training\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "np.random.seed(46)\n",
    "rn.seed(1342)\n",
    "tf.random.set_seed(62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the libraries, we will begin by reading the data generated from the Monte Carlo Simulation of the desalination model from an `csv` file. We will then remove any points where the model was found to be infeasible using the `dropna` function from pandas. Given that the simulations are on the order of 1e3, we will use the entire dataset; however, users can choose the number of samples they want to use for training the surrogate model. Finally, we will define the input and output variables for the surrogate model and extract the input and output labels from the column names of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Auto-reformer training data\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "# Reading the data from csv and sampling from it.\n",
    "# Replace mvc_data.csv with md_data.csv to train surrogate for membrane distillation.\n",
    "# One can also use a custom file and change the headers to train their own surrogate\n",
    "csv_data = pd.read_csv(\"./mvc_data.csv\")\n",
    "csv_data.dropna(inplace=True)\n",
    "csv_data = csv_data[csv_data[\"Status\"] == 1]\n",
    "data = csv_data.sample(n=len(csv_data))\n",
    "\n",
    "# Defining the input and the output columns\n",
    "input_data = data[[\"Inlet TDS (g/kg)\", \"Recovery\", \"Flow (L/s)\"]]\n",
    "output_data = data[[\"CAPEX (kUSD/year)\", \"OPEX (kUSD/year)\"]]\n",
    "\n",
    "# Define labels\n",
    "input_labels = input_data.columns\n",
    "output_labels = output_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the data to make sure the data is as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the dataframe to make sure things are in order\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to set up the neural network architecture, where we specify the activation function, optimizer, number of hidden layers, and number of nodes per layer. These are hyperparameters that should be configured by the user. We then define the loss function and the metrics for evaluating the model's performance. Since this is a regression model, we use mean squared error (MSE) as the loss function, and mean absolute error (MAE) and mean squared error (MSE) as the metrics.\n",
    "\n",
    "Next, we split the dataset into training and testing sets, using an 80/20 split, and then further split the data into input and output dataframes. Then define the length of the input and output layer of the neural network. The next step is to normalize the data using `create_normalizing_scaler` object. \n",
    "\n",
    "We create a Keras `Sequential` object and add the input layer, followed by the hidden layers, and finally the output layer. We compile the model by specifying the loss function, optimizer, and metrics. We also set up a checkpoint to save the state of the model, storing the best-known weights based on the minimum validation loss in the `.model_checkpoint.hdf5` file.\n",
    "\n",
    "Next, we fit the model using the training and validation data, specifying the number of epochs and the callback.\n",
    "\n",
    "We then determine the minimum and maximum values of each input variable in the dataset to set the bounds for the input variables. These bounds are passed to the `KerasSurrogate` object along with the neural network model, input and output labels, input and output scalers, and input bounds. Finally, we save the `KerasSurrogate` object to a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the hyper parameters to be used for training model\n",
    "activation, optimizer, n_hidden_layers, n_nodes_per_layer = \"relu\", \"Adam\", 3, 10\n",
    "loss, metrics = \"mse\", [\"mae\", \"mse\"]\n",
    "\n",
    "# Spliting the data into train and test with 80-20 split.\n",
    "data_training, data_testing = split_training_validation(data, 0.8, seed=1)\n",
    "X_train, X_test, Y_train, Y_test = (\n",
    "    data_training[input_labels],\n",
    "    data_testing[input_labels],\n",
    "    data_training[output_labels],\n",
    "    data_testing[output_labels],\n",
    ")\n",
    "\n",
    "# Defining input and output layer lengths of the neural network\n",
    "n_inputs = len(input_labels)\n",
    "n_outputs = len(output_labels)\n",
    "\n",
    "# Scaling the data using normal scaler (x-xmin)/(xmax-xmin)\n",
    "# Converting scaled data\n",
    "# to numpy from indexed series.\n",
    "input_scaler = None\n",
    "output_scaler = None\n",
    "input_scaler = OffsetScaler.create_normalizing_scaler(X_train)\n",
    "output_scaler = OffsetScaler.create_normalizing_scaler(Y_train)\n",
    "X_train = input_scaler.scale(X_train)\n",
    "Y_train = output_scaler.scale(Y_train)\n",
    "x_train = X_train.to_numpy()\n",
    "y_train = Y_train.to_numpy()\n",
    "x_test = input_scaler.scale(X_test)\n",
    "y_test = output_scaler.scale(Y_test)\n",
    "\n",
    "# Create Keras Sequential object and build neural network\n",
    "model = tf.keras.Sequential()\n",
    "model.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units=n_nodes_per_layer, input_dim=n_inputs, activation=activation\n",
    "    )\n",
    ")\n",
    "for i in range(1, n_hidden_layers):\n",
    "    model.add(tf.keras.layers.Dense(units=n_nodes_per_layer, activation=activation))\n",
    "model.add(tf.keras.layers.Dense(units=n_outputs))\n",
    "\n",
    "# Train surrogate (calls optimizer on neural network and solves for weights)\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "mcp_save = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \".model_checkpoint.hdf5\", save_best_only=True, monitor=\"val_loss\", mode=\"min\"\n",
    ")\n",
    "history = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    verbose=2,\n",
    "    epochs=500,\n",
    "    callbacks=[mcp_save],\n",
    ")\n",
    "# save model to JSON and create callable surrogate object\n",
    "xmin, xmax = [0.00, 0.05, 0], [200, 0.95, 32.00]\n",
    "input_bounds = {input_labels[i]: (xmin[i], xmax[i]) for i in range(len(input_labels))}\n",
    "\n",
    "keras_surrogate = KerasSurrogate(\n",
    "    model,\n",
    "    input_labels=list(input_labels),\n",
    "    output_labels=list(output_labels),\n",
    "    input_bounds=input_bounds,\n",
    "    input_scaler=input_scaler,\n",
    "    output_scaler=output_scaler,\n",
    ")\n",
    "keras_surrogate.save_to_folder(\"keras_surrogate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load the model from the folder and evaluate it using the entire dataset to assess the model's fit. This is done using the $R^2$ metric, where a value closer to 1 indicates a better fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model from the folder and evaluating it.\n",
    "from idaes.core.surrogate.metrics import compute_fit_metrics\n",
    "\n",
    "surr = KerasSurrogate.load_from_folder(\"Keras_surrogate\")\n",
    "y_true = data_training[output_labels]  ### The true values from the dataframe.\n",
    "y_pred = surr.evaluate_surrogate(data_training[input_labels])  ### The predicted values\n",
    "\n",
    "# Calculating the R2 score to check the fit of the model.\n",
    "y_mean = y_true.mean(axis=0)\n",
    "SST = ((y_true - y_mean) ** 2).sum(axis=0)\n",
    "SSE = ((y_true - y_pred) ** 2).sum(axis=0)\n",
    "\n",
    "r2 = 1 - SSE / SST\n",
    "\n",
    "print(f\"The training R2 scores are\")\n",
    "print(r2)\n",
    "\n",
    "y_true = data_testing[output_labels]  ### The true values from the dataframe.\n",
    "y_pred = surr.evaluate_surrogate(data_testing[input_labels])  ### The predicted values\n",
    "\n",
    "# Calculating the R2 score to check the fit of the model.\n",
    "y_mean = y_true.mean(axis=0)\n",
    "SST = ((y_true - y_mean) ** 2).sum(axis=0)\n",
    "SSE = ((y_true - y_pred) ** 2).sum(axis=0)\n",
    "\n",
    "r2 = 1 - SSE / SST\n",
    "\n",
    "print(f\"The test R2 scores are\")\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we visualize the model using the `surrogate_scatter2D` plot to examine the fit. We also use the `surrogate_parity` plot to check the deviation of the model predictions from the ground truth and the `surrogate_residual` plot to analyze the residuals. We do this for both the training set and the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_scatter2D(\n",
    "    keras_surrogate, data_training, filename=\"keras_train_scatter2D.pdf\"\n",
    ")\n",
    "surrogate_parity(keras_surrogate, data_training, filename=\"keras_train_parity.pdf\")\n",
    "surrogate_residual(keras_surrogate, data_training, filename=\"keras_train_residual.pdf\")\n",
    "\n",
    "surrogate_scatter2D(keras_surrogate, data_testing, filename=\"keras_test_scatter2D.pdf\")\n",
    "surrogate_parity(keras_surrogate, data_testing, filename=\"keras_test_parity.pdf\")\n",
    "surrogate_residual(keras_surrogate, data_testing, filename=\"keras_test_residual.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
